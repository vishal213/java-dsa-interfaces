<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Java Developer Interview Prep</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">
    <meta name="generator" content="JBake">

    <!-- Le styles -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/asciidoctor.css" rel="stylesheet">
    <link href="css/base.css" rel="stylesheet">
    <link href="css/prettify.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
    <link rel="shortcut icon" href="favicon.ico">
  </head>
  <body onload="prettyPrint()">
    <div id="wrap">

	<!-- Fixed navbar -->
    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="">JBake</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About</a></li>
            <li><a href="feed.xml">Subscribe</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown">Dropdown <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a href="#">Action</a></li>
                <li><a href="#">Another action</a></li>
                <li><a href="#">Something else here</a></li>
                <li class="divider"></li>
                <li class="dropdown-header">Nav header</li>
                <li><a href="#">Separated link</a></li>
                <li><a href="#">One more separated link</a></li>
              </ul>
            </li>
            <li><a href="archive.html">Archive</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
    <div class="container">
	<div class="page-header">
		<h1>Java Developer Interview Prep</h1>
	</div>

	<p><em>25 May 2024</em></p>

	<p><h1>Java Developer Interview Prep</h1>
<h2>Table of Contents</h2>
<ul>
<li><a href="#core-java--backend">Core Java / Backend</a></li>
<li><a href="#hashmap-vs-concurrenthashmap">HashMap vs ConcurrentHashMap</a></li>
<li><a href="#equalscontract">equals()/hashCode() Contract</a></li>
<li><a href="#immutability-patterns">Immutability Patterns</a></li>
<li><a href="#threads-vs-executors">Threads vs Executors</a></li>
<li><a href="#java-memory-model-basics">Java Memory Model Basics</a></li>
<li><a href="#gc-tuning-g1-and-zgc">GC Tuning: G1 and ZGC</a></li>
<li><a href="#java-8-11-17-features-in-practice">Java 8, 11, 17 Features in Practice</a></li>
<li><a href="#resilience-in-production">Resilience in Production</a></li>
<li><a href="#dsa--coding">DSA / Coding</a></li>
<li><a href="#arrays-and-strings">Arrays and Strings</a></li>
<li><a href="#graphs-and-trees">Graphs and Trees</a></li>
<li><a href="#dynamic-programming-staples">Dynamic Programming Staples</a></li>
<li><a href="#binary-search-patterns-and-stream-pipelines">Binary Search Patterns and Stream Pipelines</a></li>
<li><a href="#sql--database">SQL / Database</a></li>
<li><a href="#index-design-deduplication-and-window-functions">Index Design, Deduplication, and Window Functions</a></li>
<li><a href="#transactions-pagination-and-read-vs-write-models">Transactions, Pagination, and Read vs Write Models</a></li>
<li><a href="#spring--microservices">Spring / Microservices</a></li>
<li><a href="#spring-boot-auto-configuration-and-observability-basics">Spring Boot Auto-Configuration and Observability Basics</a></li>
<li><a href="#rest-design-essentials">REST Design Essentials</a></li>
<li><a href="#messaging-patterns-with-kafka-and-rabbitmq">Messaging Patterns with Kafka and RabbitMQ</a></li>
<li><a href="#observability-at-scale">Observability at Scale</a></li>
<li><a href="#system-design">System Design</a></li>
<li><a href="#upi-style-payment-service">UPI-Style Payment Service</a></li>
<li><a href="#transaction-feed-service-tstore-inspired">Transaction Feed Service (TStore Inspired)</a></li>
<li><a href="#in-app-inbox-and-alerts-bullhorn-style">In-App Inbox and Alerts (Bullhorn-Style)</a></li>
<li><a href="#job-scheduler-at-scale-clockwork-like">Job Scheduler at Scale (Clockwork-Like)</a></li>
<li><a href="#metrics-platform-for-slos">Metrics Platform for SLOs</a></li>
</ul>
<hr />
<h2>Core Java / Backend</h2>
<h3>HashMap vs ConcurrentHashMap</h3>
<ul>
<li><strong>HashMap</strong> is not thread-safe. In a multi-threaded context, concurrent modifications can corrupt the bucket structure, leading to infinite loops or lost updates. It allows one <code>null</code> key and multiple <code>null</code> values.</li>
<li><strong>ConcurrentHashMap</strong> provides thread-safe operations without global locking. It forbids <code>null</code> keys or values to avoid ambiguity in <code>get()</code> operations under race conditions. Modern implementations (Java 8+) use lock-striping and tree bins to reduce contention and avoid worst-case O(n) lookups.</li>
<li>Iterators: <code>HashMap</code> iterators fail-fast (throw <code>ConcurrentModificationException</code>). ConcurrentHashMap iterators are weakly consistent: they reflect the state at or after construction but never throw due to concurrent writes.</li>
<li>Performance: ConcurrentHashMap uses segmented locking internally. Reads are lock-free while writes synchronize on individual bins. Prefer <code>ConcurrentHashMap</code> when frequent concurrent reads and occasional writes happen, but use higher-level abstractions (<code>computeIfAbsent</code>, <code>merge</code>) to atomically update values and avoid race conditions.</li>
</ul>
<h3>equals()/_hashCode() Contract</h3>
<ul>
<li>Contract summary:</li>
</ul>
<ol>
<li>If <code>a.equals(b)</code> is true, <code>a.hashCode() == b.hashCode()</code> must hold.</li>
<li>Consistency: Repeated calls during object lifetime must return the same value if fields used are unchanged.</li>
<li>Symmetry, reflexivity, transitivity must be preserved.</li>
</ol>
<ul>
<li>Violations produce undefined behavior in hash-based collections (<code>HashSet</code>, <code>HashMap</code>). For example, forgetting to include a field in both <code>equals</code> and <code>hashCode</code> can allow duplicates or break retrieval.</li>
<li>Implementation tips:</li>
<li>Use <code>Objects.equals</code> and <code>Objects.hash</code> for null-safe comparisons.</li>
<li>Keep <code>equals</code> final unless you fully understand inheritance impacts.</li>
<li>For performance-critical classes, precompute hash codes for immutable objects to avoid repeated calculations.</li>
</ul>
<h3>Immutability Patterns</h3>
<ul>
<li>Immutable objects guarantee thread safety without synchronization because state is fixed after construction.</li>
<li>Guidelines:</li>
<li>Make fields <code>private final</code>.</li>
<li>No setters; only constructors or builders should populate data.</li>
<li>Defensive copies for mutable inputs/outputs.</li>
<li>Common patterns: <code>record</code> classes (Java 16+), <code>Collections.unmodifiableList</code>, Guava <code>ImmutableList</code>.</li>
<li>Benefits: safe sharing across threads, easier reasoning, fail-fast design. Trade-offs include potential higher allocation cost and complex object graphs requiring builders.</li>
<li>Example:</li>
</ul>
<pre><code class="language-java">public final class Money {
    private final BigDecimal amount;
    private final Currency currency;

    public Money(BigDecimal amount, Currency currency) {
        this.amount = amount.stripTrailingZeros();
        this.currency = currency;
    }

    public BigDecimal amount() {
        return amount;
    }

    public Currency currency() {
        return currency;
    }
}
</code></pre>
<h3>Threads vs Executors</h3>
<ul>
<li>Creating raw <code>Thread</code> instances is flexible but leads to brittle lifecycle management. Each <code>Thread</code> owns a stack and OS resources; frequent creation hurts performance.</li>
<li>Executors decouple task submission from execution:</li>
<li><code>ExecutorService</code> (fixed, cached, scheduled pools).</li>
<li><code>ForkJoinPool</code> for divide-and-conquer tasks.</li>
<li>Use <code>Executors.newFixedThreadPool</code> for predictable workloads, <code>newCachedThreadPool</code> for bursty but short-lived tasks (beware unbounded growth). Prefer <code>ThreadPoolExecutor</code> constructor for custom policies (queue size, rejection handler, thread factory).</li>
<li>Graceful shutdown: <code>shutdown()</code> to reject new tasks, <code>awaitTermination()</code> to block; <code>shutdownNow()</code> as best-effort interrupt. Always handle interruptions using <code>Thread.currentThread().isInterrupted()</code> checks and propagate <code>InterruptedException</code>.</li>
<li>Example implementations:</li>
</ul>
<pre><code class="language-java">public void launchWithThreads(List&lt;Runnable&gt; tasks) {
    List&lt;Thread&gt; threads = new ArrayList&lt;&gt;();
    for (Runnable task : tasks) {
        Thread worker = new Thread(task);
        worker.start();
        threads.add(worker);
    }
    for (Thread worker : threads) {
        try {
            worker.join();
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt(); // preserve interrupt status for callers
            return;
        }
    }
}
</code></pre>
<ul>
<li>Raw threads give maximum control but force manual lifecycle management and error handling.</li>
</ul>
<pre><code class="language-java">public void launchWithExecutor(List&lt;Runnable&gt; tasks) {
    ExecutorService pool = Executors.newFixedThreadPool(
            Runtime.getRuntime().availableProcessors());
    try {
        for (Runnable task : tasks) {
            pool.submit(task);
        }
    } finally {
        pool.shutdown();
    }
    try {
        if (!pool.awaitTermination(30, TimeUnit.SECONDS)) {
            pool.shutdownNow(); // interrupt lingering tasks
        }
    } catch (InterruptedException ie) {
        pool.shutdownNow();
        Thread.currentThread().interrupt();
    }
}
</code></pre>
<ul>
<li>Executors simplify pooling, queueing, and shutdown logic while letting you plug in custom rejection policies and thread factories when needed.</li>
</ul>
<h3>Java Memory Model Basics</h3>
<ul>
<li>The JMM defines happens-before relationships, ensuring visibility and ordering guarantees.</li>
<li>Key tools:</li>
<li><code>volatile</code> establishes write-read happens-before but does not ensure atomic compound operations. Use for state flags, not counters.</li>
<li><code>synchronized</code> blocks establish mutual exclusion and happens-before between unlock/lock.</li>
<li><code>java.util.concurrent</code> classes provide higher-level fences (<code>AtomicInteger</code>, <code>Lock</code>).</li>
<li>Safe publication patterns: static initializers, volatile references, final fields, thread-confined objects, and <code>CompletableFuture</code>.</li>
<li>Common pitfalls: double-checked locking requires volatile reference, <code>stop()</code> is unsafe because it violates invariants.</li>
</ul>
<h3>GC Tuning: G1 and ZGC</h3>
<ul>
<li><strong>G1 (Garbage-First) Collector</strong></li>
<li>Region-based; prioritizes regions with most garbage first.</li>
<li>Targets predictable pause times via <code>-XX:MaxGCPauseMillis</code>.</li>
<li>Useful for large heaps (multi-GB) with moderate latency requirements.</li>
<li>Tuning knobs: adjust concurrent thread counts, enable string dedup (<code>-XX:+UseStringDeduplication</code>), monitor with JVM GC logs (<code>-Xlog:gc*</code>).</li>
<li><strong>ZGC (Z Garbage Collector)</strong></li>
<li>Concurrent, region-based, uses colored pointers and load barriers to keep pauses under 10 ms even on huge heaps (multi-terabytes).</li>
<li>Ideal for low-latency services; requires recent JVM (11+ with updates, stable in 15+).</li>
<li>Minimal tuning: set reasonable heap ranges (<code>-Xms</code>, <code>-Xmx</code>), monitor using JFR (Java Flight Recorder).</li>
<li>GC tuning process:</li>
</ul>
<ol>
<li>Gather baseline metrics (allocation rate, pause times).</li>
<li>Identify longest pauses and memory pressure.</li>
<li>Adjust pause goals or heap size; consider enabling GC logging with <code>-Xlog:gc*:file=gc.log</code>.</li>
<li>Use tools like GC Easy, JMC for analysis.</li>
</ol>
<h3>Java 8, 11, 17 Features in Practice</h3>
<ul>
<li><strong>Streams (Java 8)</strong>: Express data transformations declaratively; lazy evaluation combined with terminal operations. Prefer parallel streams only when data is large, operations stateless, and splitting efficient.</li>
</ul>
<pre><code class="language-java">List&lt;String&gt; topCustomers = orders.stream()
        .filter(order -&gt; order.total() &gt; 1000)
        .sorted(Comparator.comparing(Order::total).reversed())
        .limit(10)
        .map(Order::customerId)
        .toList();
</code></pre>
<ul>
<li>Quick patterns:</li>
</ul>
<pre><code class="language-java">List&lt;String&gt; upperCase = names.stream()
        .map(String::toUpperCase)
        .sorted()
        .toList();
</code></pre>
<pre><code class="language-java">Map&lt;String, Long&gt; countByCity = users.stream()
        .collect(Collectors.groupingBy(User::city, Collectors.counting()));
</code></pre>
<pre><code class="language-java">double averageAge = users.stream()
        .mapToInt(User::age)
        .average()
        .orElse(0);
</code></pre>
<ul>
<li><strong>Optional</strong>: Fosters null-safe APIs, but avoid replacing all fields with <code>Optional</code>. Use as return type.</li>
<li><strong>Functional interfaces and method references</strong>: Encourage succinct callbacks, custom collectors.</li>
<li><strong>Java 9-11</strong>:</li>
<li><code>var</code> (Java 10) reduces verbosity while preserving static typing; use when the initializer reveals type clearly.</li>
<li>HTTP Client (Java 11) for reactive HTTP calls, supports HTTP/2 and WebSockets.</li>
<li>Local variable type inference in lambdas (Java 11) for annotations on parameters.</li>
<li><strong>Java 17</strong>:</li>
<li><code>record</code> classes provide immutable data carriers with generated accessors.</li>
<li>Sealed classes restrict inheritance hierarchies, improving pattern matching safety.</li>
<li>Enhanced <code>switch</code> expressions allow concise branching with exhaustiveness checks.</li>
<li>Text blocks for multiline strings with clear indentation control.</li>
</ul>
<pre><code class="language-java">record Customer(String id, String name) {}

String payload = &quot;&quot;&quot;
        {
          &quot;type&quot;: &quot;PING&quot;,
          &quot;timestamp&quot;: %d
        }
        &quot;&quot;&quot;.formatted(System.currentTimeMillis());
</code></pre>
<h3>Resilience in Production</h3>
<ul>
<li><strong>Retries</strong>: Useful for transient failures (timeouts, 5xx). Combine with exponential backoff and jitter to avoid thundering herds. Configure retry count per downstream dependency; avoid retrying non-idempotent operations unless supported.</li>
<li><strong>Timeouts</strong>: Every remote call requires a timeout shorter than client timeout to prevent thread exhaustion. Use <code>CompletableFuture.orTimeout</code> or <code>HttpClient</code> <code>Duration</code>.</li>
<li><strong>Circuit breakers</strong>: Trip when error rate exceeds threshold, fast-failing to allow downstream recovery (Resilience4j, Spring Cloud Circuit Breaker). Combine with fallback responses or cached data.</li>
<li>Where applied:</li>
<li>Client SDKs invoking downstream services.</li>
<li>Data access layers hitting DBs with overloaded queries.</li>
<li>External payment gateways with intermittent issues.</li>
<li>Observability: track retry counts, circuit states, timeout metrics. Document SLA/SLO targets to align with resilience thresholds.</li>
</ul>
<hr />
<h2>DSA / Coding</h2>
<h3>Arrays and Strings</h3>
<p>Key interview tasks stress time complexity, space usage, and trade-offs.</p>
<h4>Two-Sum Variants</h4>
<ul>
<li>Unsorted array with unique solution:</li>
</ul>
<pre><code class="language-java">public int[] twoSum(int[] nums, int target) {
    Map&lt;Integer, Integer&gt; indexByValue = new HashMap&lt;&gt;();
    for (int i = 0; i &lt; nums.length; i++) {
        int complement = target - nums[i];
        if (indexByValue.containsKey(complement)) {
            return new int[]{indexByValue.get(complement), i};
        }
        indexByValue.put(nums[i], i);
    }
    throw new IllegalArgumentException(&quot;No solution found&quot;);
}
</code></pre>
<ul>
<li>Sorted array variant uses two pointers to stay O(n):</li>
</ul>
<pre><code class="language-java">public int[] twoSumSorted(int[] nums, int target) {
    int left = 0, right = nums.length - 1;
    while (left &lt; right) {
        int sum = nums[left] + nums[right];
        if (sum == target) {
            return new int[]{left, right};
        } else if (sum &lt; target) {
            left++;
        } else {
            right--;
        }
    }
    throw new IllegalArgumentException(&quot;No solution found&quot;);
}
</code></pre>
<h4>Move Zeroes</h4>
<ul>
<li>Stable in-place approach with two pointers:</li>
</ul>
<pre><code class="language-java">public void moveZeroes(int[] nums) {
    int insert = 0;
    for (int num : nums) {
        if (num != 0) {
            nums[insert++] = num;
        }
    }
    while (insert &lt; nums.length) {
        nums[insert++] = 0;
    }
}
</code></pre>
<h4>Valid Anagram</h4>
<ul>
<li>Counting array for lowercase letters; for Unicode prefer <code>Map&lt;Character, Integer&gt;</code>.</li>
</ul>
<pre><code class="language-java">public boolean isAnagram(String s, String t) {
    if (s.length() != t.length()) {
        return false;
    }
    int[] counts = new int[26];
    for (int i = 0; i &lt; s.length(); i++) {
        counts[s.charAt(i) - 'a']++;
        counts[t.charAt(i) - 'a']--;
    }
    for (int count : counts) {
        if (count != 0) {
            return false;
        }
    }
    return true;
}
</code></pre>
<h4>Longest Common Prefix</h4>
<ul>
<li>Horizontal scanning avoids sorting.</li>
</ul>
<pre><code class="language-java">public String longestCommonPrefix(String[] strs) {
    if (strs.length == 0) {
        return &quot;&quot;;
    }
    String prefix = strs[0];
    for (int i = 1; i &lt; strs.length; i++) {
        while (!strs[i].startsWith(prefix)) {
            prefix = prefix.substring(0, prefix.length() - 1);
            if (prefix.isEmpty()) {
                return &quot;&quot;;
            }
        }
    }
    return prefix;
}
</code></pre>
<h3>Graphs and Trees</h3>
<ul>
<li>Choose adjacency lists for sparse graphs. Use iterative traversals when recursion depth may explode.</li>
</ul>
<h4>BFS and DFS Templates</h4>
<pre><code class="language-java">public List&lt;Integer&gt; bfs(Map&lt;Integer, List&lt;Integer&gt;&gt; graph, int start) {
    List&lt;Integer&gt; visitedOrder = new ArrayList&lt;&gt;();
    Queue&lt;Integer&gt; queue = new ArrayDeque&lt;&gt;();
    Set&lt;Integer&gt; visited = new HashSet&lt;&gt;();
    queue.add(start);
    visited.add(start);
    while (!queue.isEmpty()) {
        int node = queue.poll();
        visitedOrder.add(node);
        for (int neighbor : graph.getOrDefault(node, List.of())) {
            if (visited.add(neighbor)) {
                queue.add(neighbor);
            }
        }
    }
    return visitedOrder;
}

public List&lt;Integer&gt; dfs(Map&lt;Integer, List&lt;Integer&gt;&gt; graph, int start) {
    List&lt;Integer&gt; order = new ArrayList&lt;&gt;();
    Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;();
    Set&lt;Integer&gt; visited = new HashSet&lt;&gt;();
    stack.push(start);
    while (!stack.isEmpty()) {
        int node = stack.pop();
        if (!visited.add(node)) {
            continue;
        }
        order.add(node);
        List&lt;Integer&gt; neighbors = graph.getOrDefault(node, List.of());
        for (int i = neighbors.size() - 1; i &gt;= 0; i--) {
            stack.push(neighbors.get(i)); // push in reverse to mimic recursive DFS order
        }
    }
    return order;
}
</code></pre>
<h4>Shortest Path (Dijkstra)</h4>
<pre><code class="language-java">public Map&lt;Integer, Integer&gt; dijkstra(Map&lt;Integer, List&lt;int[]&gt;&gt; graph, int source) {
    Map&lt;Integer, Integer&gt; dist = new HashMap&lt;&gt;();
    PriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;(Comparator.comparingInt(edge -&gt; edge[1]));
    pq.add(new int[]{source, 0});
    dist.put(source, 0);

    while (!pq.isEmpty()) {
        int[] current = pq.poll();
        int node = current[0];
        int currentDist = current[1];
        if (currentDist &gt; dist.get(node)) {
            continue; // outdated entry
        }
        for (int[] edge : graph.getOrDefault(node, List.of())) {
            int neighbor = edge[0];
            int weight = edge[1];
            int newDist = currentDist + weight;
            if (newDist &lt; dist.getOrDefault(neighbor, Integer.MAX_VALUE)) {
                dist.put(neighbor, newDist);
                pq.add(new int[]{neighbor, newDist});
            }
        }
    }
    return dist;
}
</code></pre>
<h4>Tree Re-rooting Technique</h4>
<ul>
<li>Useful for computing distance sums from all nodes. Perform two DFS passes: first to compute subtree metrics, second to propagate results.</li>
</ul>
<pre><code class="language-java">public int[] sumOfDistances(int n, List&lt;List&lt;Integer&gt;&gt; tree) {
    int[] subtreeSize = new int[n];
    int[] answer = new int[n];
    Arrays.fill(subtreeSize, 1);

    dfsPost(0, -1, tree, subtreeSize, answer);
    dfsPre(0, -1, tree, subtreeSize, answer, n);
    return answer;
}

private void dfsPost(int node, int parent, List&lt;List&lt;Integer&gt;&gt; tree, int[] size, int[] answer) {
    for (int neighbor : tree.get(node)) {
        if (neighbor == parent) {
            continue;
        }
        dfsPost(neighbor, node, tree, size, answer);
        size[node] += size[neighbor];
        answer[node] += answer[neighbor] + size[neighbor];
    }
}

private void dfsPre(int node, int parent, List&lt;List&lt;Integer&gt;&gt; tree, int[] size, int[] answer, int n) {
    for (int neighbor : tree.get(node)) {
        if (neighbor == parent) {
            continue;
        }
        answer[neighbor] = answer[node] - size[neighbor] + (n - size[neighbor]);
        dfsPre(neighbor, node, tree, size, answer, n);
    }
}
</code></pre>
<h4>Disjoint Set Union (Union-Find)</h4>
<pre><code class="language-java">public static final class DisjointSetUnion {
    private final int[] parent;
    private final int[] rank;

    public DisjointSetUnion(int size) {
        parent = new int[size];
        rank = new int[size];
        for (int i = 0; i &lt; size; i++) {
            parent[i] = i;
        }
    }

    public int find(int x) {
        if (parent[x] != x) {
            parent[x] = find(parent[x]); // path compression
        }
        return parent[x];
    }

    public boolean union(int a, int b) {
        int rootA = find(a);
        int rootB = find(b);
        if (rootA == rootB) {
            return false;
        }
        if (rank[rootA] &lt; rank[rootB]) {
            parent[rootA] = rootB;
        } else if (rank[rootA] &gt; rank[rootB]) {
            parent[rootB] = rootA;
        } else {
            parent[rootB] = rootA;
            rank[rootA]++;
        }
        return true;
    }
}
</code></pre>
<h3>Dynamic Programming Staples</h3>
<h4>Longest Increasing Subsequence (O(n log n))</h4>
<pre><code class="language-java">public int lengthOfLIS(int[] nums) {
    List&lt;Integer&gt; tails = new ArrayList&lt;&gt;();
    for (int num : nums) {
        int index = Collections.binarySearch(tails, num);
        if (index &lt; 0) {
            index = -index - 1;
        }
        if (index == tails.size()) {
            tails.add(num);
        } else {
            tails.set(index, num);
        }
    }
    return tails.size();
}
</code></pre>
<ul>
<li><code>tails[i]</code> keeps the minimum possible tail of an increasing subsequence of length <code>i + 1</code>. Replacing values maintains optimal substructure.</li>
</ul>
<h4>Coin Change (Minimum Coins)</h4>
<pre><code class="language-java">public int coinChange(int[] coins, int amount) {
    int max = amount + 1;
    int[] dp = new int[amount + 1];
    Arrays.fill(dp, max);
    dp[0] = 0;

    for (int coin : coins) {
        for (int current = coin; current &lt;= amount; current++) {
            dp[current] = Math.min(dp[current], dp[current - coin] + 1);
        }
    }
    return dp[amount] &gt; amount ? -1 : dp[amount];
}
</code></pre>
<ul>
<li>Iterating coins outermost ensures unlimited usage. Using <code>max</code> sentinel avoids overflow.</li>
</ul>
<h3>Binary Search Patterns and Stream Pipelines</h3>
<h4>Binary Search Templates</h4>
<ul>
<li>Classic search:</li>
</ul>
<pre><code class="language-java">public int binarySearch(int[] nums, int target) {
    int left = 0, right = nums.length - 1;
    while (left &lt;= right) {
        int mid = left + (right - left) / 2;
        if (nums[mid] == target) {
            return mid;
        } else if (nums[mid] &lt; target) {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }
    return -1;
}
</code></pre>
<ul>
<li>First position greater or equal (lower bound):</li>
</ul>
<pre><code class="language-java">public int lowerBound(int[] nums, int target) {
    int left = 0, right = nums.length;
    while (left &lt; right) {
        int mid = left + (right - left) / 2;
        if (nums[mid] &lt; target) {
            left = mid + 1;
        } else {
            right = mid;
        }
    }
    return left;
}
</code></pre>
<ul>
<li>Pattern usage: searching answer space (e.g., minimizing max pages, rope cutting). Formulate monotonic predicate <code>P(x)</code> and find boundary where predicate transitions.</li>
</ul>
<h4>Stream Transformations</h4>
<ul>
<li>Pipeline: <code>filter</code> narrows dataset, <code>map</code> transforms, <code>reduce</code> aggregates. Ensure stateless operations for deterministic results.</li>
</ul>
<pre><code class="language-java">double totalHighValue = transactions.stream()
        .filter(tx -&gt; tx.status() == Status.SUCCESS)
        .filter(tx -&gt; tx.amount().compareTo(BigDecimal.valueOf(500)) &gt; 0)
        .map(Transaction::amount)
        .reduce(BigDecimal.ZERO, BigDecimal::add)
        .doubleValue();
</code></pre>
<ul>
<li>Use <code>Collectors.groupingBy</code> and <code>Collectors.mapping</code> for multi-level aggregates. For large data consider <code>StreamSupport</code> with Spliterators or switch to parallel streams when operations are CPU-bound and thread-safe.</li>
</ul>
<hr />
<h2>SQL / Database</h2>
<h3>Index Design, Deduplication, and Window Functions</h3>
<ul>
<li><strong>Index design</strong>:</li>
<li>B-tree indexes speed up range queries and equality filters. Composite index order matters: <code>(country, created_at)</code> serves queries filtering by <code>country</code> and ordering by <code>created_at</code>, but not the reverse unless leftmost prefix is used.</li>
<li>Trade-offs: extra write cost (index maintenance), storage overhead, potential slower bulk inserts. Use covering indexes to avoid table lookups.</li>
<li><strong>Deduplicating rows</strong>:</li>
</ul>
<pre><code class="language-sql">DELETE FROM payments p
USING (
    SELECT id, ROW_NUMBER() OVER (PARTITION BY external_id ORDER BY created_at DESC) AS rnum
    FROM payments
) dup
WHERE p.id = dup.id
  AND dup.rnum &gt; 1;
</code></pre>
<ul>
<li>Alternative: create unique constraints with <code>ON CONFLICT DO NOTHING</code> (PostgreSQL) to prevent duplicates.</li>
<li><strong>Window functions vs GROUP BY</strong>:</li>
<li><code>GROUP BY</code> collapses rows; window functions retain detail rows while computing aggregates.</li>
<li>Example: running total and rank simultaneously.</li>
</ul>
<pre><code class="language-sql">SELECT user_id,
       created_at,
       amount,
       SUM(amount) OVER (PARTITION BY user_id ORDER BY created_at) AS running_total,
       RANK() OVER (PARTITION BY user_id ORDER BY amount DESC) AS amount_rank
FROM payouts;
</code></pre>
<h3>Transactions, Pagination, and Read vs Write Models</h3>
<ul>
<li><strong>Transaction isolation</strong>:</li>
<li>Read uncommitted (rare), read committed (default), repeatable read, serializable. Understand anomalies: dirty reads, non-repeatable reads, phantom reads.</li>
<li>Postgres <code>repeatable read</code> prevents non-repeatable and phantom reads via MVCC snapshots; MySQL InnoDB's repeatable read uses next-key locks to avoid phantoms.</li>
<li>Choose isolation per use case: payments require repeatable read+ with retry logic; analytics can afford read committed.</li>
<li><strong>Pagination strategies</strong>:</li>
<li>Offset-based: <code>LIMIT 50 OFFSET 5000</code> simple but slow for large offsets and can skip rows if inserts happen.</li>
<li>Keyset pagination: use last seen key (<code>WHERE created_at &lt; ? ORDER BY created_at DESC LIMIT 50</code>) for stable, fast navigation.</li>
<li>Seek pagination works only on indexed, monotonically increasing columns.</li>
<li><strong>Read vs write models</strong>:</li>
<li>CQRS (Command Query Responsibility Segregation) splits write-optimized stores (normalized relational) from read models (denormalized caches, Elasticsearch).</li>
<li>Event sourcing feeds read models asynchronously, enabling timeline reconstruction and audit.</li>
<li>Feed services often combine write-through cache (Redis) for latest data, plus background jobs to backfill asynchronous projections.</li>
</ul>
<hr />
<h2>Spring / Microservices</h2>
<h3>Spring Boot Auto-Configuration and Observability Basics</h3>
<ul>
<li>Auto-configuration inspects classpath and configuration properties to create beans. Use <code>@SpringBootApplication</code> to trigger <code>@EnableAutoConfiguration</code>. Override defaults with <code>@ConditionalOnBean</code>, custom <code>@Configuration</code>.</li>
<li>Configuration profiles (<code>application-dev.yml</code>, <code>application-prod.yml</code>) enable environment-specific properties. Activate via <code>SPRING_PROFILES_ACTIVE</code> or command line <code>--spring.profiles.active=prod</code>.</li>
<li>Spring Boot Actuator provides endpoints like <code>/actuator/health</code>, <code>/actuator/metrics</code>. Secure them using Spring Security or network policies. Customize health indicators by implementing <code>HealthIndicator</code>.</li>
</ul>
<h3>REST Design Essentials</h3>
<ul>
<li><strong>Validation</strong>: Use <code>@Valid</code> with Bean Validation annotations (<code>@NotNull</code>, <code>@Pattern</code>). Provide constraint violation handlers to map to consistent error responses.</li>
</ul>
<pre><code class="language-java">@PostMapping(&quot;/payments&quot;)
public ResponseEntity&lt;PaymentResponse&gt; createPayment(@Valid @RequestBody PaymentRequest request) {
    Payment payment = paymentService.create(request);
    return ResponseEntity.status(HttpStatus.CREATED).body(PaymentResponse.from(payment));
}
</code></pre>
<ul>
<li><strong>Versioning</strong>: URI versioning (<code>/v1/payments</code>), header based (<code>Accept: application/vnd.company.v1+json</code>), or query param. Maintain backward compatibility; deprecate with documentation and sunset headers.</li>
<li><strong>Idempotency</strong>: Use idempotency keys (store request hash). For PUT/PATCH ensure operations update resources deterministically. When retrying POST, check if prior request succeeded before re-executing.</li>
<li><strong>Distributed workflows</strong>:</li>
<li>SAGA pattern coordinates distributed transactions via orchestrators (central controller) or choreography (events). Outbox pattern ensures messages are stored atomically with DB changes and later published safely.</li>
<li>Implement outbox using persistent tables + change data capture (Debezium) or transactional message brokers (Kafka with transaction support).</li>
</ul>
<h3>Messaging Patterns with Kafka and RabbitMQ</h3>
<ul>
<li><strong>Kafka</strong>:</li>
<li>Strong ordering within a partition. Select partition key carefully (e.g., payment ID) to guarantee order. Use idempotent producers and transactions to avoid duplicates.</li>
<li>Retries: configure <code>retries</code>, <code>acks=all</code>, and <code>enable.idempotence=true</code>. For consumers, handle poison messages with Dead Letter Topics (DLTs).</li>
<li>Consumer lag monitoring (<code>__consumer_offsets</code>) is essential. For payments, combine with exactly-once semantics using transactional IDs to write to DB and Kafka atomically.</li>
<li><strong>RabbitMQ</strong>:</li>
<li>Work queues distribute tasks (round-robin). Acknowledge messages after processing; use <code>prefetch</code> (QoS) to control in-flight count.</li>
<li>Dead-letter exchanges capture failed messages. Use delayed exchanges for retry with backoff.</li>
<li>When to choose:
<ul>
<li>Kafka for high-throughput event streaming, log aggregation, ordered events.</li>
<li>RabbitMQ for low-latency, complex routing (topic, fanout), request-response patterns.</li>
</ul>
</li>
<li>Payments flow example:</li>
</ul>
<ol>
<li>API publishes to Kafka topic <code>payment_initiated</code> with key = payment ID.</li>
<li>Orchestrator service consumes, performs risk checks, writes to DB.</li>
<li>Succeeded events published to settlement topic.</li>
<li>RabbitMQ queue handles synchronous notifications (SMS/email) because of per-message TTL and delayed retries.</li>
</ol>
<h3>Observability at Scale</h3>
<ul>
<li>Metrics: instrument via Micrometer (<code>@Timed</code>, <code>Counter</code>). Export to Prometheus/StatsD.</li>
<li>Tracing: OpenTelemetry instrumentation with Spring Cloud Sleuth (deprecated) or Micrometer Tracing. Propagate trace IDs via headers (<code>traceparent</code>).</li>
<li>Logs: structured JSON logs with <code>traceId</code>, <code>spanId</code>. Ship to ELK, Loki.</li>
<li>Scaling to 300B+ metrics/day:</li>
<li>Cardinality control: avoid unbounded label values; use exemplars selectively.</li>
<li>Metric aggregation tiers: edge agents perform pre-aggregation before shipping.</li>
<li>Sampling and adaptive aggregation for tracing and spans (tail-based sampling).</li>
<li>Storage: choose TSDB (Cortex, Mimir, Thanos) with object storage backend. Index retention policies and downsampling for cost control.</li>
<li>Alerting: define SLOs using multi-window, multi-burn rate alerting to balance sensitivity and noise.</li>
</ul>
<hr />
<h2>System Design</h2>
<h3>UPI-Style Payment Service</h3>
<ul>
<li><strong>Requirements</strong>: instant bank-to-bank transfers, high availability, real-time fraud checks, regulatory compliance (audit logs, reconciliation). Involves participants: payer app, PSP, NPCI-style switch, issuing bank, acquiring bank.</li>
<li><strong>Architecture</strong>:</li>
<li>API gateway → Auth service (device binding, OAuth) → Payment orchestrator.</li>
<li>Orchestrator leverages microservices: risk, limits, ledger, notification.</li>
<li>Message queues (Kafka) coordinate state transitions (<code>INITIATED</code>, <code>DEBIT_PENDING</code>, <code>CREDIT_PENDING</code>, <code>SETTLED</code>).</li>
<li>Use idempotency tokens per transaction; store in Redis with TTL to deduplicate.</li>
<li>Ledger service uses append-only datastore (CockroachDB, PostgreSQL with logical replication) ensuring atomic double-entry (debit payer, credit intermediary) with eventual settlement to acquiring bank.</li>
<li><strong>Consistency</strong>:</li>
<li>Two-phase commit is expensive; instead rely on eventual consistency with compensating transactions (reverse ledger entry).</li>
<li>Reconciliation jobs compare transaction logs with bank statements; mismatches escalate manual workflows.</li>
<li><strong>Scalability</strong>:</li>
<li>Partition ledger by user ID or VPA to reduce contention.</li>
<li>Employ read replicas for balance queries; keep writes on primary with synchronous replication to meet RPO.</li>
<li>Cache frequent metadata (VPA-to-account mapping) using Redis with invalidation on updates.</li>
<li><strong>Security &amp; Compliance</strong>:</li>
<li>Encrypt sensitive data at rest (HSM for keys). Use tokenization for account numbers.</li>
<li>Implement rate limiting at gateway and device binding to curb fraud.</li>
<li>Provide audit trails by persisting every state change with timestamps, actor info.</li>
</ul>
<h3>Transaction Feed Service (TStore Inspired)</h3>
<ul>
<li><strong>Goal</strong>: deliver real-time feed of transactions with pagination, filters, and backfill.</li>
<li><strong>Write path</strong>:</li>
<li>Kafka topic <code>transaction_events</code> ingested by feed builder service.</li>
<li>Denormalize into document store (Elasticsearch) optimized for search by user, status, time.</li>
<li>Maintain Redis sorted sets for latest N entries per user for fast dashboard loads.</li>
<li><strong>Read path</strong>:</li>
<li>API composes from Redis (recent) and Elastic (historical) using keyset pagination.</li>
<li>Background workers compact old feed entries into cold storage (S3 + Presto) for analytics.</li>
<li><strong>Deduplication</strong>:</li>
<li>Use event IDs with Kafka's exactly-once semantics or de-dup store (Redis bloom filter) to avoid duplicates.</li>
<li><strong>Backfill jobs</strong>:</li>
<li>Replay from Kafka using consumer groups with offsets.</li>
<li>Snapshot DB to rebuild Elastic index. Use change data capture to maintain sync.</li>
<li><strong>SLOs</strong>:</li>
<li>Freshness under 2 seconds. Monitor pipeline lag. Add circuit breaker around Elastic queries to fall back to cached summary if cluster degraded.</li>
</ul>
<h3>In-App Inbox and Alerts (Bullhorn-Style)</h3>
<ul>
<li><strong>Use case</strong>: notifications, chat-like updates, targeted alerts.</li>
<li><strong>Components</strong>:</li>
<li>Producer services publish events to Kafka topics (e.g., <code>alerts</code>, <code>messages</code>).</li>
<li>Notification service processes and fans out to WebSocket gateways, push notification service, email.</li>
<li>Store messages in scalable store (Cassandra, DynamoDB) partitioned by user ID + time bucket to support TTL and wide-row access.</li>
<li><strong>Delivery semantics</strong>:</li>
<li>Use sequence IDs per conversation to ensure ordering. Persist ack offsets per device to resume.</li>
<li><strong>Fan-out patterns</strong>:</li>
<li>Write-time fan-out for high priority (pre-compute per user). Read-time fan-out for large audiences (store broadcast messages once with membership lookup on fetch).</li>
<li><strong>Real-time channels</strong>:</li>
<li>WebSockets behind load balancer; use sticky sessions for connection state or external session store.</li>
<li>For offline delivery, utilize FCM/APNs. Include dedupe keys to avoid resending identical alerts.</li>
<li><strong>Monitoring</strong>:</li>
<li>Track delivery latency percentiles. Configure DLQs for failed template renders. Provide admin UI for replays.</li>
</ul>
<h3>Job Scheduler at Scale (Clockwork-Like)</h3>
<ul>
<li><strong>Problem</strong>: schedule millions of jobs reliably (cron, one-off, recurring).</li>
<li><strong>Architecture</strong>:</li>
<li>Control plane handles job definitions, cron parsing. Store metadata in SQL or metadata service with strong consistency.</li>
<li>Sharded scheduler workers pull upcoming jobs. Each shard owns time ranges using consistent hashing to avoid duplication.</li>
<li>Use Redis or ZooKeeper for leader election and distributed locks (e.g., Redisson). Heartbeats detect worker failures and trigger shard rebalancing.</li>
<li>Execution plane: workers enqueue tasks onto message queues (Kafka, RabbitMQ, SQS) for execution microservices.</li>
<li><strong>Reliability</strong>:</li>
<li>Persist execution logs; implement at-least-once semantics, with idempotent job handlers to avoid double runs.</li>
<li>Support backpressure: if a downstream queue is full, scheduler delays job with exponential backoff.</li>
<li><strong>Cron evaluation</strong>:</li>
<li>Precompute next fire times using libraries (Quartz). For very large volumes, bucket jobs into minute-level wheels (hierarchical timing wheels) to reduce CPU.</li>
<li><strong>APIs</strong>:</li>
<li>Provide pause/resume, override, SLA monitoring. Alert when job misses window. Offer dry-run mode with simulated schedules.</li>
</ul>
<h3>Metrics Platform for SLOs</h3>
<ul>
<li><strong>Ingestion</strong>:</li>
<li>Agents (sidecars, SDKs) push metrics to edge collectors (Prometheus remote-write, OTLP exporters). Collectors batch, compress, and forward to central ingest.</li>
<li>Use Kafka as durable buffer to decouple producers from storage. Partition by metric name for aggregation locality.</li>
<li><strong>Storage</strong>:</li>
<li>Time-series database with multi-tier: hot storage (Cortex/Mimir) for recent data; warm storage (Parquet on S3) for historical queries. Downsample older data (e.g., 5m rollups after 7 days).</li>
<li><strong>Query Layer</strong>:</li>
<li>Provide PromQL/Grafana. Implement query caching (memcached) for popular dashboards.</li>
<li><strong>SLO calculations</strong>:</li>
<li>Store error budget windows (4 hour, 30 day). Precompute rolling aggregates using Flink/Spark streaming jobs. Use burn-rate alerts to notify on fast consumption.</li>
<li><strong>Multi-tenancy and cardinality</strong>:</li>
<li>Enforce label cardinality quotas per team. Offer tooling to detect high-cardinality metrics automatically (topK by series count).</li>
<li><strong>Observability</strong>:</li>
<li>Instrument the observability stack itself (watchdog metrics). Provide anomaly detection for ingestion gaps.</li>
<li><strong>Security</strong>:</li>
<li>Tenant isolation with per-tenant API tokens. Encrypt in transit with mTLS. Apply RBAC for dashboard sharing.</li>
</ul>
<hr />
<h2>Next Steps</h2>
<ul>
<li>Practice explaining these topics aloud; interviewers value clarity and structure.</li>
<li>Implement the Java snippets in a local IDE and step through with a debugger to internalize flows.</li>
<li>Build small Spring Boot services to experiment with resilience patterns and messaging setups.</li>
<li>For system design, sketch component diagrams and rehearse trade-off discussions.</li>
</ul>
</p>

	<hr />

		</div>
		<div id="push"></div>
    </div>
    
    <div id="footer">
      <div class="container">
        <p class="muted credit">&copy; 2014 | Mixed with <a href="http://getbootstrap.com/">Bootstrap v3.1.1</a> | Baked with <a href="http://jbake.org">JBake v2.6.7</a></p>
      </div>
    </div>
    
    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/prettify.js"></script>
    
  </body>
</html>